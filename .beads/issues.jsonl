{"id":"scotty-1","title":"Unified Output System - Complete Implementation","description":"Parent epic tracking the unified output system for logs and interactive shell access. Backend is complete, CLI logs working, but some frontend features and CLI shell command remain.","design":"See docs/prds/unified-output-system.md for complete technical specification. Current status: Phase 1-3.7 and Phase 5 complete, Phase 4 partially complete.","status":"open","priority":1,"issue_type":"epic","created_at":"2025-10-25T00:58:21.660841+02:00","updated_at":"2025-10-25T00:58:21.660841+02:00"}
{"id":"scotty-10","title":"Instrument shell service with metrics","description":"Add metrics recording to ShellService for session counts, durations, errors, and timeouts.","design":"Instrument ShellService:\n- Track shell_sessions_active gauge\n- Increment shell_sessions_total counter\n- Record shell_session_duration histogram\n- Track shell_session_errors and timeouts","acceptance_criteria":"- Session metrics recorded correctly\n- Duration tracking works\n- Timeout cases tracked separately\n- Memory leak tests pass","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-25T01:28:15.964474+02:00","updated_at":"2025-10-26T18:37:15.004569+01:00","closed_at":"2025-10-26T18:37:15.004569+01:00","dependencies":[{"issue_id":"scotty-10","depends_on_id":"scotty-5","type":"parent-child","created_at":"2025-10-25T01:28:30.18956+02:00","created_by":"daemon"},{"issue_id":"scotty-10","depends_on_id":"scotty-8","type":"blocks","created_at":"2025-10-25T01:28:42.15994+02:00","created_by":"daemon"}]}
{"id":"scotty-11","title":"Instrument WebSocket connections with metrics","description":"Add metrics to WebSocket client management for connection counts, message throughput, and authentication failures.","design":"Instrument WebSocket layer:\n- Track websocket_connections_active\n- Count messages sent/received\n- Track authentication failures\n- Monitor disconnect events","acceptance_criteria":"- Connection lifecycle tracked\n- Message counters increment correctly\n- Auth failure tracking works\n- No overhead on message path","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-25T01:28:16.086525+02:00","updated_at":"2025-10-26T17:38:41.091932+01:00","closed_at":"2025-10-26T17:38:41.091932+01:00","dependencies":[{"issue_id":"scotty-11","depends_on_id":"scotty-5","type":"parent-child","created_at":"2025-10-25T01:28:30.272345+02:00","created_by":"daemon"},{"issue_id":"scotty-11","depends_on_id":"scotty-8","type":"blocks","created_at":"2025-10-25T01:28:42.23292+02:00","created_by":"daemon"}]}
{"id":"scotty-12","title":"Add observability stack to docker-compose","description":"Create docker-compose.observability.yml with OTel Collector, VictoriaMetrics, and Grafana services. Create OTel Collector configuration file.","design":"Create docker-compose.observability.yml with:\n- otel-collector service (ports 4317/4318)\n- victoriametrics service (port 8428)\n- grafana service (port 3000)\n- Create otel-collector-config.yaml with trace/metrics pipelines\n- Add Traefik labels for ddev.site domains","acceptance_criteria":"- docker-compose up works with observability stack\n- OTel Collector receives metrics on port 4317\n- VictoriaMetrics stores metrics\n- Grafana accessible at grafana.ddev.site\n- No port conflicts with existing services","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-25T01:28:16.207816+02:00","updated_at":"2025-10-25T01:38:25.612801+02:00","closed_at":"2025-10-25T01:38:25.612801+02:00","dependencies":[{"issue_id":"scotty-12","depends_on_id":"scotty-5","type":"parent-child","created_at":"2025-10-25T01:28:30.352479+02:00","created_by":"daemon"}]}
{"id":"scotty-13","title":"Create Grafana dashboards for Scotty metrics","description":"Create Grafana dashboard JSON and provisioning config showing unified output system metrics with panels for log streams, shell sessions, WebSocket connections, and tasks.","design":"Create Grafana assets:\n- grafana/provisioning/datasources/datasources.yaml (VictoriaMetrics + Jaeger)\n- grafana/provisioning/dashboards/dashboards.yaml\n- grafana/dashboards/scotty-unified-output.json\n- Panels: active streams, session durations, message rates, error rates","acceptance_criteria":"- Grafana dashboard loads automatically\n- All panels show live data\n- VictoriaMetrics datasource works\n- Dashboard is intuitive and useful\n- Export JSON to git","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-25T01:28:16.355104+02:00","updated_at":"2025-10-25T15:21:28.683425+02:00","closed_at":"2025-10-25T15:21:28.683425+02:00","dependencies":[{"issue_id":"scotty-13","depends_on_id":"scotty-5","type":"parent-child","created_at":"2025-10-25T01:28:30.434812+02:00","created_by":"daemon"},{"issue_id":"scotty-13","depends_on_id":"scotty-12","type":"blocks","created_at":"2025-10-25T01:28:42.307778+02:00","created_by":"daemon"}]}
{"id":"scotty-14","title":"Document metrics and observability setup","description":"Write documentation for setting up and using the observability stack, including metrics available, dashboard usage, and troubleshooting.","design":"Documentation:\n- README section on observability\n- docs/observability/setup.md - How to run the stack\n- docs/observability/metrics.md - Available metrics reference\n- docs/observability/dashboards.md - Dashboard guide\n- Troubleshooting common issues","acceptance_criteria":"- Setup instructions are clear and complete\n- Metrics reference documents all instruments\n- Dashboard guide has screenshots\n- Troubleshooting covers common issues\n- Updates to main README.md","status":"closed","priority":3,"issue_type":"task","created_at":"2025-10-25T01:28:16.527864+02:00","updated_at":"2025-10-26T18:43:52.600539+01:00","closed_at":"2025-10-26T18:43:52.600539+01:00","dependencies":[{"issue_id":"scotty-14","depends_on_id":"scotty-5","type":"parent-child","created_at":"2025-10-25T01:28:30.517999+02:00","created_by":"daemon"}]}
{"id":"scotty-15","title":"Upgrade OpenTelemetry dependencies to latest versions","description":"Upgrade opentelemetry, opentelemetry_sdk, and opentelemetry-otlp crates to their latest stable versions to get bug fixes, performance improvements, and new features.","design":"Check current versions in Cargo.toml workspace dependencies:\n- opentelemetry = \"0.28.0\"\n- opentelemetry_sdk = \"0.28\"\n- opentelemetry-otlp = \"0.28.0\"\n\nResearch latest versions on crates.io and upgrade incrementally. Update any API changes in:\n- scotty/src/metrics/init.rs\n- scotty/src/metrics/instruments.rs\n- scotty/src/init_telemetry.rs\n\nTest that traces and metrics still export correctly after upgrade.","acceptance_criteria":"- All opentelemetry crates upgraded to latest stable versions\n- Cargo.toml updated with new versions\n- Code compiles without errors\n- Metrics initialization works\n- Traces export successfully\n- No breaking changes in existing functionality","notes":"Successfully upgraded OpenTelemetry from 0.28 to 0.31 (latest stable version):\n\nUpgraded crates:\n- opentelemetry: 0.28.0 → 0.31.0\n- opentelemetry_sdk: 0.28 → 0.31\n- opentelemetry-otlp: 0.28.0 → 0.31.0 (added grpc-tonic feature)\n- tracing-opentelemetry: 0.29 → 0.32\n- axum-tracing-opentelemetry: 0.26.0 → 0.32.1\n- init-tracing-opentelemetry: 0.27.0 → 0.32.1\n\nAPI changes handled:\n- Fixed TraceError import (moved to opentelemetry_sdk::trace)\n- Updated init_tracerprovider imports (now in otlp::traces module)\n- Fixed TracingConfig import (moved to config module)\n- Added error conversion for init_tracerprovider\n- Fixed build_layer() result handling\n\nRemoved axum-otel-metrics due to version conflict (required 0.30 while ecosystem moved to 0.31). Implemented custom HTTP metrics middleware instead.\n\nImplementation notes:\n- Created scotty/src/metrics/http.rs with custom middleware\n- Tracks http_requests_total, http_request_duration, http_requests_active\n- Fixed telemetry_enabled flag to check for both 'traces' and 'metrics'\n- All metrics exporting correctly to VictoriaMetrics\n- Grafana dashboard updated with correct metric names\n\nCommits: bb28b5cd, 075c755b, 8e297537","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-25T01:51:52.305855+02:00","updated_at":"2025-10-25T18:54:58.935792+02:00","closed_at":"2025-10-25T18:54:58.935792+02:00","dependencies":[{"issue_id":"scotty-15","depends_on_id":"scotty-8","type":"blocks","created_at":"2025-10-25T01:51:52.306614+02:00","created_by":"daemon"}]}
{"id":"scotty-16","title":"Instrument task output streaming with metrics","description":"Add metrics recording to TaskOutputStreamingService for task counts, durations, errors, and output volume.","design":"Instrument TaskOutputStreamingService similar to LogStreamingService:\n- Track tasks_active gauge (current running tasks)\n- Track tasks_total counter (cumulative task count)\n- Add task_output_lines counter (throughput tracking)\n- Add task_errors counter (error tracking)\n- Consider task_duration histogram if tasks have clear lifecycle\n\nLocation: scotty/src/tasks/output_streaming.rs\nPattern: Use metrics::get_metrics() to access global metrics instance","acceptance_criteria":"- Metrics recorded at task start/end\n- Output lines counted\n- Error cases tracked\n- No performance degradation\n- Code compiles and tests pass","status":"closed","priority":3,"issue_type":"task","created_at":"2025-10-25T02:29:59.988921+02:00","updated_at":"2025-10-26T18:10:42.53396+01:00","closed_at":"2025-10-26T18:10:42.53396+01:00","dependencies":[{"issue_id":"scotty-16","depends_on_id":"scotty-8","type":"blocks","created_at":"2025-10-25T02:29:59.990577+02:00","created_by":"daemon"}]}
{"id":"scotty-17","title":"Add memory usage metrics","description":"Track scotty's memory usage (heap allocated, RSS, etc.) to monitor resource consumption and detect memory leaks.","design":"Add memory metrics to ScottyMetrics struct:\n- memory_heap_bytes (Gauge) - heap allocated memory\n- memory_rss_bytes (Gauge) - resident set size\n- Consider using jemalloc or system metrics crate\n\nOptions:\n1. Use `sysinfo` crate for cross-platform process metrics\n2. Use jemalloc stats if using jemalloc allocator\n3. Sample memory every 30s-60s to avoid overhead\n\nLocation: \n- Add metrics to scotty/src/metrics/instruments.rs\n- Add sampling task to scotty/src/main.rs or metrics/mod.rs\n- Record metrics periodically in background task","acceptance_criteria":"- Memory metrics exported to OTLP\n- Metrics update at reasonable interval (30-60s)\n- Minimal performance overhead\n- Works on all platforms (Linux, macOS)\n- Dashboard panel created for memory tracking","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-25T02:30:11.101617+02:00","updated_at":"2025-10-25T02:49:17.419136+02:00","closed_at":"2025-10-25T02:49:17.419136+02:00","dependencies":[{"issue_id":"scotty-17","depends_on_id":"scotty-8","type":"blocks","created_at":"2025-10-25T02:30:11.102361+02:00","created_by":"daemon"}]}
{"id":"scotty-18","title":"Add Tokio runtime metrics","description":"Track Tokio async runtime statistics like active tasks, worker threads, idle time, and task queue depth to monitor async performance.","design":"Add Tokio runtime metrics to ScottyMetrics:\n- tokio_tasks_active (Gauge) - currently spawned tasks\n- tokio_workers_count (Gauge) - number of worker threads\n- tokio_workers_idle (Gauge) - idle workers\n- tokio_tasks_spawned_total (Counter) - total tasks spawned\n\nImplementation options:\n1. Use tokio-metrics crate (official tokio metrics)\n2. Use tokio::runtime::Handle::metrics() (requires unstable features)\n3. Use tokio-console integration via console-subscriber\n\nRecommended: tokio-metrics crate\n- Add tokio-metrics to Cargo.toml\n- Create background task to sample runtime metrics\n- Record every 10-30s\n\nLocation:\n- Add metrics to scotty/src/metrics/instruments.rs\n- Add tokio metrics sampler in scotty/src/metrics/tokio.rs\n- Spawn sampling task in main.rs after runtime creation","acceptance_criteria":"- Tokio runtime metrics exported to OTLP\n- Metrics show active tasks and worker state\n- Minimal performance overhead\n- Code compiles with stable Rust\n- Dashboard panel created for runtime monitoring","status":"closed","priority":3,"issue_type":"task","created_at":"2025-10-25T02:30:22.11133+02:00","updated_at":"2025-10-25T16:17:30.994921+02:00","closed_at":"2025-10-25T16:17:30.994921+02:00","dependencies":[{"issue_id":"scotty-18","depends_on_id":"scotty-8","type":"blocks","created_at":"2025-10-25T02:30:22.112175+02:00","created_by":"daemon"}]}
{"id":"scotty-19","title":"Add TaskManager metrics for task tracking","description":"Instrument TaskManager to track number of tasks, their states (running/finished/failed), durations, and success rates.","design":"Add TaskManager metrics to ScottyMetrics struct:\n- tasks_total (Counter) - Total tasks created\n- tasks_by_state (Gauge) - Current tasks grouped by state labels:\n  * state=\"running\"\n  * state=\"finished\"\n  * state=\"failed\"\n- task_duration_seconds (Histogram) - Task execution time\n- task_failures_total (Counter) - Failed tasks counter\n\nImplementation approach:\n1. Add metrics to scotty/src/metrics/instruments.rs\n2. Instrument scotty/src/tasks/manager.rs:\n   - add_task(): increment tasks_total, update state gauge\n   - set_task_finished(): update state gauges, record duration histogram\n   - Optional: background sampler every 30s to sync gauges with HashMap state\n\nData sources:\n- TaskManager.processes HashMap size = total active tasks\n- TaskDetails.state: Running | Finished | Failed\n- TaskDetails.start_time + finish_time for duration calculation\n- TaskDetails.last_exit_code for success/failure tracking","acceptance_criteria":"- Task metrics exported to OTLP\n- Metrics accurately reflect task states\n- Duration tracking works correctly\n- Failed vs successful tasks distinguishable\n- Dashboard panels created for task monitoring\n- No performance degradation","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-25T02:37:07.454119+02:00","updated_at":"2025-10-25T15:29:19.632177+02:00","closed_at":"2025-10-25T15:29:19.632177+02:00","dependencies":[{"issue_id":"scotty-19","depends_on_id":"scotty-8","type":"blocks","created_at":"2025-10-25T02:37:07.454889+02:00","created_by":"daemon"}]}
{"id":"scotty-2","title":"Implement app:shell CLI command in scottyctl","description":"Add the app:shell command to scottyctl CLI. Backend ShellService is fully implemented and tested, but the CLI command is missing. Need to create scottyctl/src/commands/apps/shell.rs with WebSocket-based terminal integration.","design":"- Create ShellCommand struct in cli.rs\n- Implement WebSocket-based shell handler in commands/apps/shell.rs\n- Add TTY resize handling and raw terminal mode\n- Support interactive input/output with proper escape sequences\n- Reuse AuthenticatedWebSocket pattern from logs command","acceptance_criteria":"- scottyctl app:shell myapp web opens interactive shell\n- Terminal escape sequences work correctly\n- Ctrl+C and Ctrl+D handled properly\n- Session cleanup on disconnect\n- Help text and examples documented","status":"in_progress","priority":1,"issue_type":"feature","created_at":"2025-10-25T00:58:21.798713+02:00","updated_at":"2025-10-26T18:37:31.697764+01:00","dependencies":[{"issue_id":"scotty-2","depends_on_id":"scotty-1","type":"parent-child","created_at":"2025-10-25T00:58:32.439042+02:00","created_by":"daemon"}]}
{"id":"scotty-20","title":"Add AppList metrics for application monitoring","description":"Instrument SharedAppList to track number of apps, their states (stopped/starting/running/etc), services per app, and health check age.","design":"Add AppList metrics to ScottyMetrics struct:\n- apps_total (Gauge) - Total number of managed apps\n- apps_by_status (Gauge) - Apps grouped by status labels:\n  * status=\"stopped\"\n  * status=\"starting\"\n  * status=\"running\"\n  * status=\"creating\"\n  * status=\"destroying\"\n  * status=\"unsupported\"\n- app_services_count (Histogram) - Distribution of services per app\n- app_last_check_age_seconds (Histogram) - Time since last health check\n\nImplementation approach:\n1. Add metrics to scotty/src/metrics/instruments.rs\n2. Instrument scotty-core/src/apps/shared_app_list.rs:\n   - add_app() / remove_app(): update apps_total\n   - Background sampler task (30-60s interval):\n     * Iterate apps HashMap\n     * Count by status\n     * Sample services.len() distribution\n     * Calculate last_checked age\n3. Spawn sampler task in scotty/src/main.rs or via AppState\n\nData sources:\n- SharedAppList.apps HashMap size = total apps\n- AppData.status: Stopped | Starting | Running | Creating | Destroying | Unsupported\n- AppData.services.len() = service count per app\n- AppData.last_checked timestamp for age calculation\n\nNote: Requires access to SharedAppList from metrics, may need to pass via background task or add to AppState.","acceptance_criteria":"- App metrics exported to OTLP\n- Metrics accurately reflect app states\n- Status distribution correct\n- Service count distribution tracked\n- Health check age visible\n- Dashboard panels created for app monitoring\n- Minimal performance overhead from sampling","status":"closed","priority":2,"issue_type":"task","created_at":"2025-10-25T02:37:26.668911+02:00","updated_at":"2025-10-26T17:20:02.191752+01:00","closed_at":"2025-10-26T17:20:02.191752+01:00","dependencies":[{"issue_id":"scotty-20","depends_on_id":"scotty-8","type":"blocks","created_at":"2025-10-25T02:37:26.669776+02:00","created_by":"daemon"}]}
{"id":"scotty-21","title":"Add Traefik to observability docker-compose for .ddev.site URLs","description":"The observability stack uses Traefik labels for routing (grafana.ddev.site, jaeger.ddev.site, vm.ddev.site) but requires external Traefik to be running from apps/traefik. This creates a dependency and extra setup step.\n\nAdd Traefik service to observability/docker-compose.yml so the stack works out-of-the-box without requiring apps/traefik to be running.","design":"Options:\n1. Add Traefik service to observability/docker-compose.yml\n   - Include network configuration\n   - Configure dashboard access\n   - Ensure no port conflicts with main Traefik\n\n2. Share network with existing apps/traefik\n   - Create external network\n   - Less duplication but still requires apps/traefik\n\nRecommended: Option 1 - self-contained observability stack\n\nImplementation:\n- Add Traefik service to observability/docker-compose.yml\n- Use different ports than apps/traefik (80/443 vs 8080/8443)\n- Configure proxy network\n- Update README.md to mention this works standalone","acceptance_criteria":"- grafana.ddev.site, jaeger.ddev.site, vm.ddev.site work without apps/traefik running\n- cd observability \u0026\u0026 docker-compose up -d brings up full working stack\n- No port conflicts with apps/traefik\n- Documentation updated","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-25T02:59:32.096433+02:00","updated_at":"2025-10-25T02:59:32.096433+02:00"}
{"id":"scotty-22","title":"Refactor metrics instrumentation to use dedicated helper functions","description":"Review and refactor existing metrics code in log streaming and other services to move metrics recording into dedicated helper functions, keeping business logic clean and separated from instrumentation code.","design":"Pattern established in scotty-19:\n- Create dedicated helper functions like `record_task_added_metrics()` and `record_task_finished_metrics()`\n- Move all `if let Some(m) = metrics::get_metrics()` blocks into these helpers\n- Keep business logic methods focused on their primary responsibility\n\nFiles to review and refactor:\n- scotty/src/docker/services/logs.rs (log streaming metrics)\n- scotty/src/docker/services/shell.rs (shell session metrics - if implemented)\n- Any other services with inline metrics code\n\nBenefits:\n- Cleaner separation of concerns\n- Easier to test business logic without metrics\n- Consistent metrics instrumentation pattern\n- Better code readability","status":"closed","priority":3,"issue_type":"chore","created_at":"2025-10-25T15:28:09.285832+02:00","updated_at":"2025-10-25T16:07:00.885661+02:00","closed_at":"2025-10-25T16:07:00.885661+02:00"}
{"id":"scotty-23","title":"Reduce cloning overhead by wrapping AppData in Arc","description":"AppData is cloned on every access from SharedAppList. Wrapping AppData in Arc would make cloning cheap (just reference count increment) instead of copying all nested structures.","design":"Location: scotty-core/src/apps/shared_app_list.rs:56-58\n\nCurrent code clones entire AppData structure on every get_app() call. AppData contains multiple nested structures (containers, services, settings) making clones expensive.\n\nProposed solution:\n```rust\npub type SharedAppData = Arc\u0026lt;AppData\u0026gt;;\n\npub async fn get_app(\u0026amp;self, app_name: \u0026amp;str) -\u0026gt; Option\u0026lt;SharedAppData\u0026gt; {\n    let t = self.apps.read().await;\n    t.get(app_name).map(Arc::clone)  // Only clones Arc, not data\n}\n```\n\nImpact: Major performance improvement for app data access paths\nEffort: 2-4 hours","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-26T21:21:10.34027+01:00","updated_at":"2025-10-26T21:21:10.34027+01:00","labels":["performance","refactoring"]}
{"id":"scotty-24","title":"Add timeout to Docker command execution","description":"Docker commands currently have no timeout and could hang indefinitely if Docker daemon becomes unresponsive.","design":"Location: scotty/src/docker/docker_compose.rs:88\n\nCurrent code:\n```rust\nlet output = cmd.output()?;  // No timeout\n```\n\nProposed solution:\n```rust\nuse tokio::time::timeout;\n\nlet output = timeout(\n    Duration::from_secs(300),  // 5 minute timeout\n    tokio::process::Command::new(\"docker-compose\")\n        .args(command)\n        .output()\n).await??;\n```\n\nImpact: Prevents indefinite hangs on Docker failures\nEffort: 1-2 hours","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-26T21:21:10.406194+01:00","updated_at":"2025-10-26T21:21:10.406194+01:00","labels":["docker","stability"]}
{"id":"scotty-25","title":"Improve authorization error handling in Casbin enforcement","description":"Authorization service silently denies access on Casbin errors using unwrap_or(false), potentially hiding system issues.","design":"Location: scotty/src/services/authorization/service.rs:182\n\nCurrent code:\n```rust\nlet result = enforcer\n    .enforce(vec![user, app, action_str])\n    .unwrap_or(false);\n```\n\nProposed solution:\n```rust\nlet result = enforcer\n    .enforce(vec![user, app, action_str])\n    .map_err(|e| {\n        error!(\"Casbin enforce error: {}\", e);\n        crate::metrics::authorization::record_error();\n    })\n    .unwrap_or(false);\n```\n\nImpact: Better visibility into authorization failures\nEffort: 30 minutes","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-26T21:21:10.471975+01:00","updated_at":"2025-10-26T21:21:10.471975+01:00","labels":["observability","security"]}
{"id":"scotty-26","title":"Fix wildcard dependency for tracing-subscriber","description":"tracing-subscriber uses wildcard version \"*\" which prevents reproducible builds.","design":"Location: Cargo.toml:36\n\nCurrent: `tracing-subscriber = \"*\"`\nReplace with: `tracing-subscriber = \"0.3\"`\n\nImpact: Reproducible builds\nEffort: 5 minutes","status":"closed","priority":1,"issue_type":"chore","assignee":"claude","created_at":"2025-10-26T21:21:10.53848+01:00","updated_at":"2025-10-26T22:00:48.493544+01:00","closed_at":"2025-10-26T22:00:48.493544+01:00","labels":["dependencies"]}
{"id":"scotty-27","title":"Wrap large configuration structures in Arc","description":"Settings struct contains large nested HashMaps (blueprints, registries) that get cloned unnecessarily. Wrapping them in Arc would reduce clone overhead.","design":"Location: scotty-core/src/settings/apps.rs\n\nSettings contains large, rarely modified structures that are cloned when Settings is cloned.\n\nProposed solution:\n```rust\n#[derive(Clone)]\npub struct Apps {\n    pub root_folder: String,\n    pub blueprints: Arc\u0026lt;HashMap\u0026lt;String, AppBlueprint\u0026gt;\u0026gt;,  // Large, rarely modified\n    // ...\n}\n```\n\nImpact: Reduce Settings clone overhead\nEffort: 1-2 hours","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-26T21:21:10.606337+01:00","updated_at":"2025-10-26T21:21:10.606337+01:00","labels":["performance","refactoring"]}
{"id":"scotty-28","title":"Consider DashMap for WebSocket client management","description":"WebSocket client HashMap uses Arc\u0026lt;Mutex\u0026lt;HashMap\u0026gt;\u0026gt; which causes lock contention during broadcasts. DashMap provides lock-free reads.","design":"Location: scotty/src/api/websocket/messaging.rs\n\nCurrent implementation uses mutex-protected HashMap which can cause contention during broadcasts to multiple clients.\n\nProposed solution:\n```rust\nuse dashmap::DashMap;\n\npub struct WebSocketMessenger {\n    clients: Arc\u0026lt;DashMap\u0026lt;Uuid, WebSocketClient\u0026gt;\u0026gt;,\n}\n```\n\nBenefits:\n- Lock-free concurrent reads\n- Automatic sharding for better concurrency\n- Reduced contention during broadcasts\n\nImpact: Reduced lock contention during broadcasts\nEffort: 2-3 hours","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-26T21:21:10.680572+01:00","updated_at":"2025-10-26T21:21:10.680572+01:00","labels":["performance","websocket"]}
{"id":"scotty-29","title":"Split large authorization service file into submodules","description":"The authorization service.rs file is 754 lines and could be split into more focused submodules for better maintainability.","design":"Location: scotty/src/services/authorization/service.rs (754 lines)\n\nProposed structure:\n- service.rs - Core service and main authorization logic\n- permissions.rs - Permission checking and management\n- scopes.rs - Scope-related operations\n- roles.rs - Role management\n- assignments.rs - Assignment handling\n\nImpact: Better code organization and maintainability\nEffort: 2-4 hours","status":"open","priority":1,"issue_type":"chore","created_at":"2025-10-26T21:21:10.754914+01:00","updated_at":"2025-10-26T21:21:10.754914+01:00","labels":["maintainability","refactoring"]}
{"id":"scotty-3","title":"Frontend container log viewer UI","description":"Add UI to view container logs in the web frontend. Backend log streaming API is complete with WebSocket support, but frontend has no log viewer component.","design":"- Create log viewer component similar to unified-output.svelte\n- Add WebSocket handlers for LogLineReceived/LogStreamStarted/LogStreamEnded messages\n- Add log viewer page or modal accessible from app detail page\n- Support follow mode, timestamps, line limits\n- Reuse webSocketStore.ts infrastructure","acceptance_criteria":"- Can view historical logs for any service\n- Follow mode for real-time streaming\n- Toggle timestamps on/off\n- Auto-scroll control\n- Copy logs to clipboard\n- Integration from app detail page","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-10-25T00:58:21.90526+02:00","updated_at":"2025-10-25T01:06:56.395577+02:00","closed_at":"2025-10-25T01:06:56.395577+02:00","dependencies":[{"issue_id":"scotty-3","depends_on_id":"scotty-1","type":"parent-child","created_at":"2025-10-25T00:58:32.507014+02:00","created_by":"daemon"}]}
{"id":"scotty-30","title":"Add metrics for clone operations in hot paths","description":"Add tracing/metrics to performance-critical clone operations to identify actual hotspots with real usage data.","design":"Add instrumentation to measure clone operations in:\n- AppData access patterns\n- Settings propagation\n- State machine handler contexts\n\nUse tracing spans with timing information to identify which clones actually impact performance in production.\n\nThis data will help prioritize which clone operations to optimize first.\n\nImpact: Data-driven optimization decisions\nEffort: 2-3 hours","status":"open","priority":1,"issue_type":"task","created_at":"2025-10-26T21:21:10.829701+01:00","updated_at":"2025-10-26T21:21:10.829701+01:00","labels":["observability","performance"]}
{"id":"scotty-31","title":"Refactor test code organization and extract common helpers","description":"Extract common test helpers and utilities into a dedicated test utilities module to reduce duplication across test files.","design":"Current state: Test code has duplicated setup/teardown logic and helper functions across multiple test files.\n\nProposed:\n- Create `scotty/tests/common/mod.rs` for shared test utilities\n- Extract common fixtures (test users, apps, configurations)\n- Create builder patterns for test data\n- Consolidate mock setup functions\n\nImpact: Reduce test code duplication, easier to maintain tests\nEffort: 4-6 hours","status":"open","priority":1,"issue_type":"chore","created_at":"2025-10-26T21:21:10.90688+01:00","updated_at":"2025-10-26T21:21:10.90688+01:00","labels":["maintainability","testing"]}
{"id":"scotty-32","title":"Upgrade ts-rs from 10.0 to 11.0","description":"ts-rs has a major version update available (10.0 → 11.0) that should be evaluated and applied. This affects TypeScript type generation.","design":"Location: scotty-types/Cargo.toml:21\n\nCurrent: ts-rs = { version = \"10.0\", features = [\"chrono-impl\", \"uuid-impl\"] }\nTarget: ts-rs = { version = \"11.0\", features = [\"chrono-impl\", \"uuid-impl\"] }\n\nSteps:\n1. Review ts-rs 11.0 changelog for breaking changes\n2. Update version in scotty-types/Cargo.toml\n3. Run TypeScript generation and verify output\n4. Test that generated TypeScript types are compatible with frontend\n5. Update any code that uses ts-rs macros if needed\n\nImpact: May change generated TypeScript types\nEffort: 2-4 hours","status":"closed","priority":3,"issue_type":"chore","assignee":"claude","created_at":"2025-10-26T22:07:52.621782+01:00","updated_at":"2025-10-26T22:14:26.407903+01:00","closed_at":"2025-10-26T22:14:26.407903+01:00","labels":["dependencies","typescript"]}
{"id":"scotty-33","title":"Upgrade oauth2 from 4.4 to 5.0","description":"oauth2 crate has a major version update available (4.4 → 5.0). This affects OAuth2 authentication flow in the API.","design":"Location: scotty/Cargo.toml:61\n\nCurrent: oauth2 = \"4.4\"\nTarget: oauth2 = \"5.0\"\n\nSteps:\n1. Review oauth2 5.0 changelog and migration guide\n2. Identify breaking changes in API\n3. Update version in scotty/Cargo.toml\n4. Update OAuth2 implementation code to match new API\n5. Test OAuth2 authentication flows thoroughly\n6. Verify token generation and validation still works\n\nImpact: May require changes to OAuth2 authentication implementation\nEffort: 3-5 hours","status":"open","priority":3,"issue_type":"chore","created_at":"2025-10-26T22:08:00.856298+01:00","updated_at":"2025-10-26T22:08:00.856298+01:00","labels":["dependencies","oauth","security"]}
{"id":"scotty-34","title":"Upgrade rustls ecosystem from 0.21 to 0.23","description":"rustls, tokio-rustls, and hyper-rustls have major version updates available (0.21 → 0.23, 0.24 → 0.26, 0.24 → 0.27). These need coordinated updates for TLS support.","design":"Current versions (transitive dependencies):\n- rustls: 0.21.12 → 0.23.34\n- tokio-rustls: 0.24.1 → 0.26.4  \n- hyper-rustls: 0.24.2 → 0.27.7\n\nThese are currently pulled in transitively through reqwest and other dependencies.\n\nSteps:\n1. Review rustls 0.23 changelog for breaking changes\n2. Check if reqwest needs update to support rustls 0.23\n3. Update any direct dependencies on rustls/tokio-rustls\n4. Ensure all TLS connections still work properly\n5. Test HTTPS endpoints and WebSocket over TLS\n6. Verify certificate validation still works\n\nImpact: TLS/SSL implementation changes, better security and performance\nEffort: 4-6 hours\n\nNote: This may require updating other HTTP-related crates in coordination.","status":"open","priority":3,"issue_type":"chore","created_at":"2025-10-26T22:08:11.918721+01:00","updated_at":"2025-10-26T22:08:11.918721+01:00","labels":["dependencies","security","tls"]}
{"id":"scotty-35","title":"Upgrade http/hyper ecosystem to v1.x","description":"The http and hyper crates have major version updates (http 0.2 → 1.3, hyper 0.14 → 1.7, http-body 0.4 → 1.0). This is a coordinated ecosystem upgrade.","design":"Current versions (transitive dependencies):\n- http: 0.2.12 → 1.3.1\n- http-body: 0.4.6 → 1.0.1\n- hyper: 0.14.32 → 1.7.0\n- h2: 0.3.27 → 0.4.12\n\nThese are foundational HTTP crates used by axum, reqwest, and other dependencies.\n\nSteps:\n1. Review hyper 1.0 migration guide and breaking changes\n2. Check if current versions of axum/reqwest support hyper 1.x\n3. May need to update axum, reqwest, tower-http in coordination\n4. Update any direct usage of http/hyper types in code\n5. Test all HTTP endpoints (REST API, WebSocket, etc.)\n6. Verify middleware and error handling still works\n7. Run full integration test suite\n\nImpact: Major HTTP stack upgrade, affects all network communication\nEffort: 6-10 hours\n\nNote: This is a significant upgrade that touches the core HTTP stack. Should be done carefully with comprehensive testing. May need to wait for ecosystem crates to fully support hyper 1.x.","status":"open","priority":3,"issue_type":"chore","created_at":"2025-10-26T22:08:24.871193+01:00","updated_at":"2025-10-26T22:08:24.871193+01:00","labels":["dependencies","http","infrastructure"]}
{"id":"scotty-36","title":"Fix hardcoded localhost in OAuth callback URL","description":"Location: scotty/src/oauth/handlers.rs:456-459\n\nCurrently hardcoded as:\n```rust\nformat!(\n    \"http://localhost:21342/oauth/callback?session_id={}\",\n    oauth_session_id\n)\n```\n\nImpact:\n- OAuth flow will fail in deployed/production environments\n- Security vulnerability: forces HTTP instead of HTTPS\n- Not configurable for different deployment scenarios","design":"Use base_url from configuration or extract from request:\n\n```rust\nlet base_url = state.settings.api.base_url\n    .as_ref()\n    .unwrap_or(\u0026\"http://localhost:21342\".to_string());\nformat!(\"{}/oauth/callback?session_id={}\", base_url, oauth_session_id)\n```\n\nShould respect HTTPS in production environments and be configurable via settings.","notes":"## Review Analysis\n\n**Status: VALID ISSUE** (with corrections)\n\n### Current Code (handlers.rs:451-460)\n```rust\nlet frontend_url =\n    if let Some(frontend_callback) = \u0026session.frontend_callback_url {\n        format!(\"{}?session_id={}\", frontend_callback, oauth_session_id)\n    } else {\n        // Fallback to frontend OAuth callback page if no frontend callback specified\n        format!(\n            \"http://localhost:21342/oauth/callback?session_id={}\",\n            oauth_session_id\n        )\n    };\n```\n\n### Problems Confirmed\n1. ✅ Hardcoded localhost:21342 in fallback URL\n2. ✅ Hardcoded HTTP (not HTTPS)\n3. ✅ No configuration option to override\n\n### Correction to Original Issue\nThe suggested fix using `state.settings.api.base_url` is **incorrect** - there is NO base_url field in ApiServer (see scotty-core/src/settings/api_server.rs:51-64).\n\n### Actual Available Fields\n- `settings.api.bind_address` (e.g., \"0.0.0.0:21342\") - not suitable for public URLs\n- `settings.api.oauth.redirect_url` (e.g., \"/oauth2/start\") - relative path only\n- `settings.api.oauth.oauth2_proxy_base_url` - optional, for proxy scenarios\n\n### Recommended Fix\n1. Add new field to OAuthSettings: `frontend_base_url: Option\u003cString\u003e`\n2. Use this field to construct the fallback URL\n3. Default to localhost:21342 for development\n4. Require configuration in production deployments","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-27T01:12:22.061033+01:00","updated_at":"2025-10-28T21:43:28.827779+01:00","closed_at":"2025-10-28T21:43:28.827779+01:00","labels":["oauth","production","security"]}
{"id":"scotty-37","title":"Implement session cleanup for expired OAuth sessions","description":"Location: scotty/src/oauth/handlers.rs (session storage)\n\nOAuth sessions stored in Arc\u0026lt;Mutex\u0026lt;HashMap\u0026gt;\u0026gt; are never cleaned up. Expired sessions remain in memory indefinitely.\n\nImpact:\n- Memory leak vulnerability\n- Especially problematic under attack scenarios with repeated device flow polling\n- Sessions accumulate without bounds","design":"Implement background task to clean expired sessions:\n\n```rust\n// Spawn cleanup task on startup\ntokio::spawn(async move {\n    let mut interval = tokio::time::interval(Duration::from_secs(300));\n    loop {\n        interval.tick().await;\n        cleanup_expired_sessions(\u0026oauth_session_store).await;\n    }\n});\n```\n\nTask should:\n1. Run periodically (e.g., every 5 minutes)\n2. Check session expiration timestamps\n3. Remove expired sessions from the HashMap\n4. Log cleanup statistics for monitoring","notes":"## Review Analysis\n\n**Status: VALID ISSUE**\n\n### Confirmed Problems\n1. ✅ Three session stores exist with no automatic cleanup:\n   - DeviceFlowStore (device flow sessions)\n   - WebFlowStore (web flow sessions)  \n   - OAuthSessionStore (temporary sessions for token exchange)\n\n2. ✅ Sessions ARE checked for expiration when accessed:\n   - handlers.rs:341-347 (web flow expiration check)\n   - handlers.rs:522-526 (OAuth session expiration check)\n   - device_flow.rs:78-81 (device flow expiration check)\n\n3. ✅ Sessions are removed ONLY when:\n   - Successfully exchanged/completed (handlers.rs:517-519, 427-429)\n   - Accessed and found to be expired (returns error but doesn't remove)\n\n4. ✅ **Memory Leak Confirmed**: \n   - Expired sessions that are never accessed again stay in memory forever\n   - Device flow sessions from abandoned flows accumulate\n   - Web flow sessions from abandoned authorization attempts accumulate\n   - Attack scenario: repeated device flow requests create unbounded memory growth\n\n### Session Lifetimes\n- DeviceFlowSession: ~10 minutes (typical device flow TTL)\n- WebFlowSession: 10 minutes (handlers.rs:206)\n- OAuthSession: 5 minutes (handlers.rs:441)\n\n### Recommended Fix Validated\nOriginal suggestion is sound:\n1. Spawn background cleanup task on startup\n2. Run every 5 minutes (or configurable interval)\n3. Lock each store and remove expired sessions\n4. Log cleanup statistics for monitoring\n\n### Additional Consideration\nEven when sessions are accessed and found expired, they should be removed from the store at that time to prevent re-checking the same expired session.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-27T01:12:22.103898+01:00","updated_at":"2025-10-28T21:43:28.869193+01:00","closed_at":"2025-10-28T21:43:28.869193+01:00","labels":["memory-leak","oauth","security"]}
{"id":"scotty-38","title":"Protect PKCE verifier with secrecy wrapper","description":"Location: scotty/src/oauth/mod.rs:40-48 (WebFlowSession struct)\n\nCurrently PKCE verifier is stored as plain String:\n```rust\npub struct WebFlowSession {\n    pub csrf_token: String,\n    pub pkce_verifier: String,  // Base64 encoded for storage\n    pub redirect_url: String,\n    pub frontend_callback_url: Option\u003cString\u003e,\n    pub expires_at: SystemTime,\n}\n```\n\nImpact:\n- PKCE verifier is sensitive cryptographic material that remains in memory unprotected\n- Can be exposed in memory dumps, crash reports, or debug logs\n- Not zeroized when dropped, leaving sensitive data in memory\n- Information leakage vulnerability","design":"Use secrecy crate to protect PKCE verifier:\n\n```rust\nuse secrecy::{Secret, ExposeSecret};\n\npub struct WebFlowSession {\n    pub csrf_token: String,\n    pub pkce_verifier: Secret\u003cString\u003e,  // ✅ Protected and zeroized\n    pub redirect_url: String,\n    pub frontend_callback_url: Option\u003cString\u003e,\n    pub expires_at: SystemTime,\n}\n```\n\nChanges needed:\n1. Add secrecy dependency if not already present\n2. Wrap pkce_verifier in Secret\u0026lt;String\u0026gt;\n3. Update all code that accesses pkce_verifier to use .expose_secret()\n4. Update serialization/deserialization to work with Secret wrapper\n5. Consider protecting csrf_token similarly (also sensitive material)\n\nAffected code locations:\n- scotty/src/oauth/handlers.rs:203 (storing verifier)\n- scotty/src/oauth/handlers.rs:386 (decoding verifier)","notes":"## Review Analysis\n\n**Status: VALID ISSUE**\n\n### Confirmed Problems\n1. ✅ The secrecy crate is **already a dependency** of the project (Cargo.toml)\n2. ✅ Project has custom `MaskedSecret` wrapper (scotty-core/src/utils/secret.rs)\n3. ✅ `MaskedSecret` is used extensively for sensitive data (passwords, API keys, etc.)\n4. ✅ PKCE verifier is stored as **plain String** in WebFlowSession (oauth/mod.rs:43)\n5. ✅ No usage of Secret/MaskedSecret in OAuth handlers currently\n\n### Why This Matters\n- PKCE verifier is sensitive cryptographic material\n- Zeroized on drop prevents memory leaks of sensitive data\n- Prevents exposure in debug logs, crash dumps, error messages\n- Project already has the infrastructure - just not using it here\n\n### Existing Infrastructure\nThe project already has:\n```rust\npub struct MaskedSecret(SecretString);\n```\n\nFeatures:\n- Memory protection via secrecy crate\n- Partial masking in Debug output\n- Requires explicit `.expose_secret()` calls\n- Serde support for serialization/deserialization\n\n### Recommendation Validated\nShould use `MaskedSecret` instead of plain `String`:\n\n```rust\nuse crate::utils::secret::MaskedSecret;\n\npub struct WebFlowSession {\n    pub csrf_token: String,  // Consider protecting this too\n    pub pkce_verifier: MaskedSecret,  // ✅ Use MaskedSecret\n    pub redirect_url: String,\n    pub frontend_callback_url: Option\u003cString\u003e,\n    pub expires_at: SystemTime,\n}\n```\n\nThis is a security hardening issue that's easy to fix since the infrastructure already exists.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-10-27T01:15:02.48335+01:00","updated_at":"2025-10-28T21:52:12.347154+01:00","closed_at":"2025-10-28T21:52:12.347154+01:00","labels":["cryptography","oauth","security"]}
{"id":"scotty-39","title":"Use constant-time comparison for bearer token validation","description":"Location: scotty/src/api/auth_core.rs:173-182 (find_token_identifier function)\n\nCurrent implementation uses standard string equality:\n```rust\nfn find_token_identifier(shared_app_state: \u0026SharedAppState, token: \u0026str) -\u003e Option\u0026lt;String\u0026gt; {\n    for (identifier, configured_token) in \u0026shared_app_state.settings.api.bearer_tokens {\n        if configured_token == token {  // ❌ NOT constant-time\n            return Some(identifier.clone());\n        }\n    }\n    None\n}\n```\n\nImpact:\n- Standard `==` comparison is NOT constant-time\n- Timing attack vulnerability: attacker can measure response times to enumerate valid tokens\n- Each character comparison that matches takes slightly longer than mismatches\n- Allows token prefix guessing through timing analysis\n- Theoretical but real security risk, especially on high-latency networks","design":"Use constant-time comparison via subtle crate:\n\n```rust\nuse subtle::ConstantTimeEq;\n\nfn find_token_identifier(shared_app_state: \u0026SharedAppState, token: \u0026str) -\u003e Option\u0026lt;String\u0026gt; {\n    for (identifier, configured_token) in \u0026shared_app_state.settings.api.bearer_tokens {\n        // Compare in constant time to prevent timing attacks\n        if configured_token.as_bytes().ct_eq(token.as_bytes()).into() {\n            return Some(identifier.clone());\n        }\n    }\n    None\n}\n```\n\nChanges needed:\n1. Add subtle crate to workspace dependencies in Cargo.toml:\n   ```toml\n   subtle = \"2.6\"\n   ```\n2. Add use statement in auth_core.rs\n3. Replace `==` with `ct_eq()` for token comparison\n4. Consider applying same fix to other sensitive comparisons (CSRF tokens, etc.)\n\nNote: Project already uses secrecy/zeroize crates (Cargo.toml:91-92), so adding subtle aligns with security-conscious dependency choices.","notes":"## Review Analysis\n\n**Status: VALID ISSUE**\n\n### Confirmed Timing-Vulnerable Comparisons\n\n1. ✅ **Bearer token comparison** (auth_core.rs:176)\n   ```rust\n   if configured_token == token {  // NOT constant-time\n   ```\n\n2. ✅ **CSRF token comparison** (oauth/handlers.rs:375)\n   ```rust\n   if csrf_part != session.csrf_token {  // NOT constant-time\n   ```\n\n### Why This Matters\n\nStandard string comparison (`==`, `!=`) in Rust:\n- Short-circuits on first mismatch\n- Returns faster for early mismatches vs late mismatches\n- Allows attacker to measure timing differences\n- Can reveal information about token prefixes\n\n### Attack Scenario\n\n1. Attacker tries multiple tokens with different prefixes\n2. Measures response time for each attempt\n3. Tokens that match more characters take slightly longer\n4. Gradually narrows down valid token through timing analysis\n5. Especially effective over high-latency networks or with statistical analysis\n\n### Exploit Difficulty\n\n- **Low** on localhost/LAN (nanosecond differences hard to measure)\n- **Medium** over internet (network jitter masks timing)\n- **High** with many requests for statistical analysis\n- **Real risk** in production environments with persistent attackers\n\n### Project Security Posture\n\nGood indicators:\n- Already uses `secrecy` crate (Cargo.toml:91)\n- Already uses `zeroize` crate (Cargo.toml:92)\n- Has custom `MaskedSecret` implementation\n- Shows awareness of security best practices\n\nMissing:\n- No `subtle` crate for constant-time comparisons\n- No timing-safe string comparison\n\n### Recommendation\n\nPriority 2 is appropriate (medium priority):\n- Not immediately exploitable like RCE\n- Requires sophisticated attack\n- But should be fixed before production\n- Easy fix with subtle crate","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-10-27T01:16:33.692257+01:00","updated_at":"2025-10-28T21:43:28.90992+01:00","closed_at":"2025-10-28T21:43:28.90992+01:00","labels":["authentication","security","timing-attack"]}
{"id":"scotty-4","title":"Frontend interactive shell UI with xterm.js","description":"Add interactive shell terminal to web frontend using xterm.js. Backend ShellService is complete with WebSocket support, but frontend has no shell UI.","design":"- Integrate xterm.js for terminal emulation\n- WebSocket handlers for ShellSession* messages\n- TTY resize support on window resize\n- Copy/paste support\n- Terminal settings (font size, theme)\n- Shell session management UI (list, create, terminate)\n- Session timeout indicator","acceptance_criteria":"- Can open interactive shell to any service from web UI\n- Terminal emulation works correctly (colors, escape sequences)\n- Copy/paste functional\n- Terminal resizes properly\n- Session list shows active shells\n- Clean session termination\n- Security: requires Shell permission","status":"open","priority":2,"issue_type":"feature","created_at":"2025-10-25T00:58:22.023108+02:00","updated_at":"2025-10-25T00:58:22.023108+02:00","dependencies":[{"issue_id":"scotty-4","depends_on_id":"scotty-1","type":"parent-child","created_at":"2025-10-25T00:58:32.576268+02:00","created_by":"daemon"},{"issue_id":"scotty-4","depends_on_id":"scotty-3","type":"related","created_at":"2025-10-25T00:58:39.410575+02:00","created_by":"daemon"}]}
{"id":"scotty-40","title":"Add rate limiting to OAuth endpoints to prevent DoS attacks","description":"Location: scotty/src/api/router.rs:415-419 (public OAuth routes)\n\nCurrently exposed OAuth endpoints without rate limiting:\n```rust\nlet public_router = Router::new()\n    .route(\"/oauth/device\", post(start_device_flow))\n    .route(\"/oauth/device/token\", post(poll_device_token))\n    .route(\"/oauth/authorize\", get(start_authorization_flow))\n    .route(\"/api/oauth/callback\", get(handle_oauth_callback))\n    .route(\"/oauth/exchange\", post(exchange_session_for_token))\n```\n\nImpact:\n- **Unlimited device flow polling** - attacker can spam /oauth/device/token causing CPU/memory exhaustion\n- **Device flow session exhaustion** - combined with memory leak (scotty-37), creates unbounded memory growth\n- **Brute force attacks** - unlimited attempts on /api/oauth/callback with different state parameters\n- **DoS vulnerability** - no cost to attacker for making thousands of requests\n- **Resource exhaustion** - especially problematic with existing session cleanup issues\n\nAttack Scenarios:\n1. Spam /oauth/device to create thousands of sessions (memory leak)\n2. Poll /oauth/device/token continuously for DoS\n3. Brute force /api/oauth/callback trying to guess valid session_id values\n4. Exhaust backend OAuth provider quota through unlimited authorize requests","design":"Add rate limiting middleware using tower-governor:\n\n```rust\nuse tower_governor::{\n    governor::GovernorConfigBuilder, \n    key_extractor::SmartIpKeyExtractor,\n    GovernorLayer\n};\n\n// Create rate limit configuration for OAuth endpoints\nlet oauth_rate_limit = Box::new(\n    GovernorConfigBuilder::default()\n        .per_second(5)           // 5 requests per second\n        .burst_size(10)          // Allow bursts up to 10\n        .key_extractor(SmartIpKeyExtractor)  // Rate limit by IP\n        .finish()\n        .unwrap()\n);\n\n// Apply to OAuth routes\nlet oauth_router = Router::new()\n    .route(\"/oauth/device\", post(start_device_flow))\n    .route(\"/oauth/device/token\", post(poll_device_token))\n    .route(\"/oauth/authorize\", get(start_authorization_flow))\n    .route(\"/api/oauth/callback\", get(handle_oauth_callback))\n    .route(\"/oauth/exchange\", post(exchange_session_for_token))\n    .layer(GovernorLayer { config: oauth_rate_limit });\n```\n\nRecommended limits:\n- Device flow creation: 5/sec, burst 10 (expensive operation)\n- Device token polling: 10/sec, burst 20 (spec recommends 5s interval)\n- Authorization flow: 10/sec, burst 20 (interactive user flow)\n- Callback: 10/sec, burst 20 (one-time use per flow)\n- Exchange: 10/sec, burst 20 (one-time use per session)\n\nDependencies needed:\n```toml\ntower-governor = \"0.4\"\n```\n\nConsider:\n1. Different rate limits for different endpoints\n2. Rate limit by IP address (SmartIpKeyExtractor)\n3. Configurable via settings for different deployment scenarios\n4. Proper HTTP 429 (Too Many Requests) responses with Retry-After header","notes":"## Review Analysis\n\n**Status: VALID ISSUE - HIGH PRIORITY**\n\n### Confirmed Vulnerable Endpoints\n\nAll OAuth endpoints are PUBLIC with NO rate limiting (router.rs:415-419):\n\n1. ✅ `POST /oauth/device` - Create device flow session (unauthenticated)\n2. ✅ `POST /oauth/device/token` - Poll for token (unauthenticated)\n3. ✅ `GET /oauth/authorize` - Start web flow (unauthenticated)\n4. ✅ `GET /api/oauth/callback` - OAuth callback (unauthenticated)\n5. ✅ `POST /oauth/exchange` - Exchange session for token (unauthenticated)\n\n### Attack Impact Analysis\n\n**Critical Combination with scotty-37**:\n- No rate limiting + memory leak = catastrophic DoS\n- Attacker can create unlimited device flow sessions\n- Each session remains in memory forever (no cleanup)\n- Memory grows unbounded until OOM crash\n\n**Verified Attack Vectors**:\n\n1. **Device Flow DoS** (Most Severe)\n   ```bash\n   # Spam device flow creation - each creates uncleaned session\n   while true; do curl -X POST http://target/oauth/device; done\n   ```\n   - Creates new HashMap entries indefinitely\n   - No cleanup (scotty-37)\n   - No rate limiting\n   - Server crashes within minutes\n\n2. **Polling DoS**\n   ```bash\n   # Poll device token endpoint continuously\n   while true; do curl -X POST http://target/oauth/device/token?device_code=fake; done\n   ```\n   - CPU exhaustion from continuous processing\n   - Session lookups on every request\n\n3. **Brute Force Session IDs**\n   ```bash\n   # Try to guess valid session_ids\n   for i in {1..100000}; do \n     curl \"http://target/api/oauth/callback?session_id=$RANDOM\"\n   done\n   ```\n   - No limit on failed attempts\n   - Can enumerate valid sessions\n\n### OAuth Spec Guidance\n\nRFC 8628 (Device Authorization Grant):\n- Recommends minimum polling interval of 5 seconds\n- Server SHOULD enforce rate limiting\n- Can return `slow_down` error if client polls too fast\n\nCurrent implementation:\n- ❌ No enforcement of polling interval\n- ❌ No `slow_down` error response\n- ❌ No rate limiting at all\n\n### Severity Justification\n\n**Priority 1 (Critical)** is appropriate because:\n1. Unauthenticated endpoints (low barrier to attack)\n2. Compounds existing memory leak (scotty-37)\n3. Easy to exploit (simple curl loops)\n4. Can crash server in minutes\n5. No monitoring/alerting capability without rate limits","status":"open","priority":1,"issue_type":"bug","created_at":"2025-10-27T01:20:09.30647+01:00","updated_at":"2025-10-27T01:20:28.806483+01:00","labels":["dos","oauth","rate-limiting","security"],"dependencies":[{"issue_id":"scotty-40","depends_on_id":"scotty-37","type":"related","created_at":"2025-10-27T01:20:28.856487+01:00","created_by":"daemon"}]}
{"id":"scotty-41","title":"Add OpenTelemetry metrics for OAuth session monitoring","description":"Add comprehensive metrics for OAuth session management to enable monitoring in Grafana and detect issues like memory leaks, excessive polling, or DoS attacks.\n\nCurrently there is NO visibility into:\n- Number of active OAuth sessions (device flow, web flow, temporary sessions)\n- Session creation/cleanup rates\n- Session expiration patterns\n- OAuth endpoint usage patterns\n- Memory pressure from session accumulation\n\nThis makes it impossible to:\n- Detect memory leaks (scotty-37) in production\n- Monitor DoS attacks (scotty-40) in real-time\n- Track session lifecycle health\n- Alert on abnormal session accumulation\n- Optimize session cleanup intervals\n\nImpact:\n- No observability into OAuth session health\n- Cannot detect memory leak until OOM crash\n- Cannot detect DoS attacks until service degradation\n- No data for capacity planning","design":"Add OAuth metrics following existing OpenTelemetry pattern in scotty/src/metrics/instruments.rs:\n\n```rust\npub struct ScottyMetrics {\n    // ... existing metrics ...\n    \n    // OAuth session metrics\n    pub oauth_device_sessions_active: Gauge\u0026lt;i64\u0026gt;,\n    pub oauth_web_sessions_active: Gauge\u0026lt;i64\u0026gt;,\n    pub oauth_temp_sessions_active: Gauge\u0026lt;i64\u0026gt;,\n    pub oauth_sessions_created_total: Counter\u0026lt;u64\u0026gt;,\n    pub oauth_sessions_expired_total: Counter\u0026lt;u64\u0026gt;,\n    pub oauth_sessions_completed_total: Counter\u0026lt;u64\u0026gt;,\n    pub oauth_session_age_seconds: Histogram\u0026lt;f64\u0026gt;,\n    pub oauth_cleanup_operations: Counter\u0026lt;u64\u0026gt;,\n    pub oauth_cleanup_removed_count: Histogram\u0026lt;f64\u0026gt;,\n    \n    // OAuth endpoint metrics\n    pub oauth_device_flow_requests: Counter\u0026lt;u64\u0026gt;,\n    pub oauth_device_token_polls: Counter\u0026lt;u64\u0026gt;,\n    pub oauth_authorize_requests: Counter\u0026lt;u64\u0026gt;,\n    pub oauth_callback_requests: Counter\u0026lt;u64\u0026gt;,\n    pub oauth_exchange_requests: Counter\u0026lt;u64\u0026gt;,\n    \n    // OAuth error metrics\n    pub oauth_expired_session_errors: Counter\u0026lt;u64\u0026gt;,\n    pub oauth_session_not_found_errors: Counter\u0026lt;u64\u0026gt;,\n    pub oauth_authorization_pending: Counter\u0026lt;u64\u0026gt;,\n    pub oauth_rate_limit_hits: Counter\u0026lt;u64\u0026gt;,\n}\n```\n\nImplementation locations:\n1. Add metrics to ScottyMetrics::new() (scotty/src/metrics/instruments.rs)\n2. Create scotty/src/metrics/oauth.rs for sampling function\n3. Instrument oauth/handlers.rs to record metrics:\n   - Session creation: increment active gauge + created counter\n   - Session cleanup: decrement active gauge + expired/completed counter\n   - Session access: record age histogram\n   - Endpoint calls: increment request counters\n   - Errors: increment error counters\n\nSampling function (called periodically):\n```rust\npub fn sample_oauth_metrics(oauth_state: \u0026amp;OAuthState, metrics: \u0026amp;ScottyMetrics) {\n    // Sample current session counts\n    let device_count = oauth_state.device_flow_store.lock().unwrap().len() as i64;\n    let web_count = oauth_state.web_flow_store.lock().unwrap().len() as i64;\n    let temp_count = oauth_state.session_store.lock().unwrap().len() as i64;\n    \n    metrics.oauth_device_sessions_active.record(device_count, \u0026amp;[]);\n    metrics.oauth_web_sessions_active.record(web_count, \u0026amp;[]);\n    metrics.oauth_temp_sessions_active.record(temp_count, \u0026amp;[]);\n}\n```\n\nGrafana dashboard queries:\n- `scotty_oauth_device_sessions_active` - Track memory leak\n- `rate(scotty_oauth_device_flow_requests[5m])` - Detect DoS\n- `scotty_oauth_sessions_active - scotty_oauth_sessions_completed` - Session leak rate\n- `rate(scotty_oauth_expired_session_errors[1m])` - Cleanup effectiveness\n\nAlerts:\n- OAuth sessions \u0026gt; 1000: Possible DoS or memory leak\n- Session creation rate \u0026gt; 100/min: Possible attack\n- Expired errors \u0026gt; 10/min: Cleanup not working\n\nChecklist:\n- OAuth session metrics added to ScottyMetrics\n- Metrics initialized in ScottyMetrics::new()\n- oauth/handlers.rs instrumented at key points\n- Sampling function added and scheduled\n- Metrics visible in Grafana/OTLP collector\n- Documentation for metric meanings","notes":"## Justification\n\nThis feature enables detection and monitoring of the security issues identified in scotty-37 and scotty-40.\n\n### Why Metrics Are Critical\n\nWithout metrics, the OAuth issues are **invisible** until catastrophic failure:\n\n**Memory Leak Detection (scotty-37)**:\n- Current: Service crashes with OOM, no warning\n- With metrics: Alert when `oauth_device_sessions_active \u003e 1000`\n- Can track growth rate: `rate(oauth_sessions_created[5m]) - rate(oauth_sessions_completed[5m])`\n\n**DoS Attack Detection (scotty-40)**:\n- Current: CPU exhaustion with no visibility into cause\n- With metrics: Alert on `rate(oauth_device_flow_requests[1m]) \u003e 100`\n- Can identify attack patterns before service degradation\n\n**Session Cleanup Effectiveness**:\n- Track if cleanup task (scotty-37 fix) is working\n- Monitor `oauth_cleanup_removed_count` histogram\n- Alert if expired sessions accumulate\n\n### Metrics Coverage\n\n**19 proposed metrics** across 3 categories:\n\n1. **Session lifecycle** (9 metrics):\n   - Active counts per store type (device/web/temp)\n   - Creation/expiration/completion counters\n   - Session age distribution\n   - Cleanup operation stats\n\n2. **Endpoint usage** (5 metrics):\n   - Request counts per OAuth endpoint\n   - Identifies hotspots and abuse patterns\n\n3. **Error tracking** (4 metrics):\n   - Expired session attempts\n   - Session not found errors\n   - Authorization pending responses\n   - Rate limit hits\n\n### Grafana Dashboard Example\n\n```\nPanel 1: OAuth Sessions Active\n  - oauth_device_sessions_active (gauge)\n  - oauth_web_sessions_active (gauge)\n  - oauth_temp_sessions_active (gauge)\n\nPanel 2: Session Growth Rate\n  - rate(oauth_sessions_created_total[5m])\n  - rate(oauth_sessions_completed_total[5m])\n  - Difference indicates leak rate\n\nPanel 3: OAuth Traffic\n  - rate(oauth_device_flow_requests[5m])\n  - rate(oauth_device_token_polls[5m])\n  - Spike detection for DoS\n\nPanel 4: Error Rates\n  - rate(oauth_expired_session_errors[1m])\n  - rate(oauth_session_not_found_errors[1m])\n```\n\n### Alert Rules\n\n```yaml\n- alert: OAuthMemoryLeak\n  expr: oauth_device_sessions_active \u003e 1000\n  for: 5m\n  \n- alert: OAuthDoSAttack\n  expr: rate(oauth_device_flow_requests[1m]) \u003e 100\n  for: 2m\n  \n- alert: OAuthCleanupFailing\n  expr: rate(oauth_expired_session_errors[5m]) \u003e 10\n  for: 10m\n```\n\n### Implementation Effort\n\n**Low effort** (~2-3 hours):\n- Follows existing pattern (see instruments.rs)\n- Project already uses OpenTelemetry\n- Just needs instrumentation at key points\n\n### Priority Justification\n\nPriority 1 because:\n- Essential for monitoring security fixes (scotty-37, scotty-40)\n- Enables proactive detection before failures\n- Low implementation cost with high value\n- Required for production readiness of OAuth feature","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-10-27T01:23:33.915902+01:00","updated_at":"2025-10-28T21:43:28.951336+01:00","closed_at":"2025-10-28T21:43:28.951336+01:00","labels":["metrics","monitoring","oauth","observability"],"dependencies":[{"issue_id":"scotty-41","depends_on_id":"scotty-37","type":"related","created_at":"2025-10-27T01:23:55.5438+01:00","created_by":"daemon"},{"issue_id":"scotty-41","depends_on_id":"scotty-40","type":"related","created_at":"2025-10-27T01:23:55.585957+01:00","created_by":"daemon"}]}
{"id":"scotty-42","title":"Implement token caching for OAuth OIDC validation","description":"Location: scotty/src/api/auth_core.rs:151\n\nCurrent implementation validates OIDC token with provider on EVERY request:\n```rust\nmatch oauth_state.client.validate_oidc_token(token).await {\n    Ok(oidc_user) =\u003e {\n        debug!(\"OAuth token validated for user...\");\n        Some(CurrentUser { /* ... */ })\n    }\n    Err(e) =\u003e {\n        warn!(\"OAuth token validation failed: {}\", e);\n        None\n    }\n}\n```\n\nImpact:\n- **High latency**: Network roundtrip to OIDC provider on every API request (100-500ms typical)\n- **OIDC provider rate limits**: Can hit provider rate limits under load\n- **Scalability bottleneck**: Cannot scale horizontally without hitting provider limits\n- **Unnecessary load**: Most tokens are valid for hours, but validated thousands of times\n- **Cost**: Some OIDC providers charge per validation request\n- **Poor user experience**: Adds latency to every authenticated API call\n\nPerformance impact example:\n- Without cache: 1000 req/sec = 1000 OIDC validations/sec = provider rate limit\n- With cache (1hr TTL): 1000 req/sec ≈ 1-10 OIDC validations/sec = no rate limit","design":"Implement in-memory token cache with configurable TTL:\n\n```rust\nuse std::time::{Duration, Instant};\nuse tokio::sync::RwLock;\n\npub struct TokenCache {\n    cache: Arc\u0026lt;RwLock\u0026lt;HashMap\u0026lt;String, CachedToken\u0026gt;\u0026gt;\u0026gt;,\n    ttl: Duration,\n}\n\nstruct CachedToken {\n    user: OidcUser,\n    cached_at: Instant,\n    expires_at: Instant,\n}\n\nimpl TokenCache {\n    pub fn new(ttl: Duration) -\u003e Self {\n        Self {\n            cache: Arc::new(RwLock::new(HashMap::new())),\n            ttl,\n        }\n    }\n    \n    pub async fn validate_with_cache(\n        \u0026amp;self, \n        token: \u0026amp;str,\n        validator: impl Fn(\u0026amp;str) -\u003e Result\u0026lt;OidcUser\u0026gt;\n    ) -\u003e Result\u0026lt;OidcUser\u0026gt; {\n        // Try cache first (read lock)\n        {\n            let cache = self.cache.read().await;\n            if let Some(cached) = cache.get(token) {\n                if Instant::now() \u0026lt; cached.expires_at {\n                    debug!(\"Token cache HIT\");\n                    return Ok(cached.user.clone());\n                }\n            }\n        }\n        \n        debug!(\"Token cache MISS - validating with provider\");\n        \n        // Cache miss or expired - validate with provider\n        let user = validator(token).await?;\n        \n        // Update cache (write lock)\n        {\n            let mut cache = self.cache.write().await;\n            cache.insert(token.to_string(), CachedToken {\n                user: user.clone(),\n                cached_at: Instant::now(),\n                expires_at: Instant::now() + self.ttl,\n            });\n        }\n        \n        Ok(user)\n    }\n    \n    // Background cleanup of expired entries\n    pub async fn cleanup_expired(\u0026amp;self) {\n        let mut cache = self.cache.write().await;\n        let now = Instant::now();\n        cache.retain(|_, cached| now \u0026lt; cached.expires_at);\n    }\n}\n```\n\nIntegration in auth_core.rs:\n```rust\npub async fn authorize_oauth_user_native(\n    shared_app_state: SharedAppState,\n    auth_header: \u0026amp;str,\n) -\u003e Option\u0026lt;CurrentUser\u0026gt; {\n    let token = auth_header.strip_prefix(\"Bearer \")?;\n    \n    let oauth_state = shared_app_state.oauth_state.as_ref()?;\n    \n    // Use token cache instead of direct validation\n    match oauth_state.token_cache\n        .validate_with_cache(token, |t| {\n            oauth_state.client.validate_oidc_token(t)\n        })\n        .await\n    {\n        Ok(oidc_user) =\u003e Some(CurrentUser { /* ... */ }),\n        Err(e) =\u003e {\n            warn!(\"OAuth token validation failed: {}\", e);\n            None\n        }\n    }\n}\n```\n\nConfiguration:\n```yaml\napi:\n  oauth:\n    token_cache_ttl: 3600  # 1 hour default\n    token_cache_cleanup_interval: 300  # 5 minutes\n```\n\nConsiderations:\n1. **Cache invalidation**: Cannot invalidate on user logout (distributed system problem)\n   - Acceptable trade-off: token expires naturally after TTL\n   - Alternative: Add revocation list for critical invalidations\n\n2. **Memory usage**: Tokens accumulate in cache\n   - Solution: Background cleanup task removes expired entries\n   - Limit: Max cache size with LRU eviction\n\n3. **Security**: Cached tokens continue working during TTL even if revoked\n   - Mitigation: Short TTL (5-15 minutes recommended)\n   - Balance: security vs performance\n\n4. **Metrics**: Track cache hit/miss rates\n   - `oauth_token_cache_hits`\n   - `oauth_token_cache_misses`  \n   - `oauth_token_cache_size`\n\n5. **Testing**: Ensure cache respects expiration\n   - Unit tests for cache logic\n   - Integration tests with token refresh","notes":"## Verification Analysis\n\n**Status: VALID ISSUE - CONFIRMED**\n\n### Confirmed Network Call on Every Request\n\nThe `validate_oidc_token` implementation (device_flow.rs:221-256) makes an HTTP request to the OIDC provider's `/oauth/userinfo` endpoint on EVERY validation:\n\n```rust\npub async fn validate_oidc_token(\u0026self, access_token: \u0026str) -\u003e Result\u003cOidcUser, OAuthError\u003e {\n    let user_url = format!(\"{}/oauth/userinfo\", self.oidc_issuer_url);\n    let response = self\n        .http_client\n        .inner()\n        .get(\u0026user_url)              // ❌ HTTP GET on every request\n        .bearer_auth(access_token)\n        .send()\n        .await?;\n    \n    // Parse and return user info\n}\n```\n\n### Call Sites - Every Authenticated Request\n\n1. **auth_core.rs:151** - Called on EVERY OAuth-authenticated API request\n2. **handlers.rs:113** - Device flow token exchange (less frequent, acceptable)\n3. **handlers.rs:423** - Web flow callback (once per flow, acceptable)\n\n### Performance Impact Calculation\n\n**Without caching**:\n- 1 API request = 1 OIDC validation = 1 network roundtrip (100-500ms)\n- 100 requests/sec = 100 validations/sec\n- 1000 requests/sec = 1000 validations/sec → rate limit territory\n\n**With caching (1 hour TTL)**:\n- First request with token = 1 validation (cache miss)\n- Next 3600 seconds of requests = 0 validations (cache hits)\n- 1000 requests/sec ≈ 0.3 validations/sec (assuming user sessions)\n\n**Cache hit ratio estimate**:\n- Average user session: 30 minutes\n- API calls per session: 100-1000\n- Expected cache hit rate: 99%+\n\n### Real-World Latency Impact\n\nWithout cache:\n```\nAPI Request → Auth Check (150ms OIDC) → Handler (50ms) = 200ms total\n```\n\nWith cache (hit):\n```\nAPI Request → Auth Check (0.1ms cache) → Handler (50ms) = 50ms total\n```\n\n**4x latency improvement** for authenticated requests.\n\n### OIDC Provider Constraints\n\n**GitLab** (common OIDC provider):\n- Rate limit: 600 requests/min per IP\n- Without cache: 10 req/sec max per instance\n- With cache: Effectively unlimited\n\n**Keycloak**:\n- Default: 30 requests/sec per realm\n- Without cache: Cannot scale beyond 30 req/sec\n- With cache: Unlimited API throughput\n\n### Security Trade-offs\n\n**Concern**: Cached tokens continue working after revocation\n\n**Mitigation strategies**:\n\n1. **Short TTL** (recommended: 5-15 minutes)\n   - Balance: performance vs revocation latency\n   - 5 min = max 5 min delay on token revocation\n   - Still 99%+ cache hit rate\n\n2. **Token revocation list** (optional enhancement)\n   ```rust\n   struct TokenCache {\n       cache: HashMap\u003cString, CachedToken\u003e,\n       revoked: HashSet\u003cString\u003e,  // Explicitly revoked tokens\n   }\n   ```\n\n3. **JTI-based caching** (best practice)\n   - Cache by JWT ID (jti) instead of full token\n   - Smaller memory footprint\n   - Same security properties\n\n### Memory Usage Estimate\n\n**Per cached token**:\n- Token string: ~500 bytes (JWT)\n- User info: ~200 bytes\n- Metadata: ~50 bytes\n- **Total: ~750 bytes/token**\n\n**Capacity**:\n- 1000 active users = 750 KB\n- 10000 active users = 7.5 MB\n- 100000 active users = 75 MB\n\n**With cleanup**: Old tokens removed every 5 minutes, memory stable.\n\n### Implementation Priority\n\n**Priority 2 (Medium)** is appropriate because:\n- Impacts performance, not security\n- Workaround: Scale horizontally (but expensive)\n- Should fix before production at scale\n- Not blocking MVP deployment\n\n### Recommended Configuration\n\n```yaml\napi:\n  oauth:\n    token_cache_enabled: true\n    token_cache_ttl_seconds: 300        # 5 minutes (security-performance balance)\n    token_cache_max_size: 10000         # Prevent unbounded growth\n    token_cache_cleanup_interval: 300   # 5 minutes\n```\n\n---\n\n## ⚠️ CRITICAL: Token Cache Cleanup Required\n\n**DO NOT FORGET**: Token cache cleanup is **mandatory** to prevent memory leak!\n\n### Why Cleanup Is Critical\n\nWithout periodic cleanup, the token cache will accumulate expired entries indefinitely, creating a memory leak similar to scotty-37 (OAuth session leak).\n\n**Memory leak scenario**:\n1. Tokens expire after TTL (e.g., 5 minutes)\n2. Expired tokens remain in HashMap\n3. New tokens added continuously\n4. Cache grows unbounded → OOM crash\n\n### Required Cleanup Implementation\n\n**1. Background cleanup task** (spawn on startup):\n```rust\n// In app initialization\ntokio::spawn(async move {\n    let mut interval = tokio::time::interval(Duration::from_secs(300)); // 5 minutes\n    loop {\n        interval.tick().await;\n        token_cache.cleanup_expired().await;\n        \n        // Log cleanup stats\n        let size = token_cache.size().await;\n        debug!(\"Token cache cleanup completed, current size: {}\", size);\n    }\n});\n```\n\n**2. Passive cleanup** (on access):\n```rust\npub async fn validate_with_cache(\u0026self, token: \u0026str) -\u003e Result\u003cOidcUser\u003e {\n    // Check cache\n    if let Some(cached) = self.cache.read().await.get(token) {\n        if Instant::now() \u003c cached.expires_at {\n            return Ok(cached.user.clone());\n        }\n        // Expired - remove immediately (passive cleanup)\n        self.cache.write().await.remove(token);\n    }\n    \n    // Validate and cache...\n}\n```\n\n**3. Size-based eviction** (LRU for safety):\n```rust\nconst MAX_CACHE_SIZE: usize = 10_000;\n\nasync fn insert_with_limit(\u0026mut self, token: String, cached: CachedToken) {\n    if self.cache.len() \u003e= MAX_CACHE_SIZE {\n        // Evict oldest entry (LRU)\n        if let Some(oldest_key) = self.find_oldest_entry() {\n            self.cache.remove(\u0026oldest_key);\n            warn!(\"Token cache size limit reached, evicted oldest entry\");\n        }\n    }\n    self.cache.insert(token, cached);\n}\n```\n\n### Cleanup Verification\n\n**Add metrics** (scotty-41 integration):\n- `oauth_token_cache_size` - Current cache size\n- `oauth_token_cache_cleanup_count` - Tokens removed per cleanup\n- `oauth_token_cache_evictions` - Size-based evictions\n\n**Add alerts**:\n```yaml\n- alert: TokenCacheGrowingUnbounded\n  expr: oauth_token_cache_size \u003e 50000\n  for: 10m\n  annotations:\n    summary: \"Token cache cleanup may not be working\"\n```\n\n### Testing Checklist\n\n- [ ] Cleanup task spawned on startup\n- [ ] Expired tokens removed by background task\n- [ ] Passive cleanup on access works\n- [ ] Size limit enforced (LRU eviction)\n- [ ] Metrics track cache size over time\n- [ ] Load test confirms stable memory usage\n- [ ] No memory growth over 24-hour period\n\n### Relationship to scotty-37\n\nThis issue is similar to scotty-37 (OAuth session cleanup):\n- Same pattern: HashMap without cleanup → memory leak\n- Same solution: Background cleanup task + metrics\n- **Must not repeat the same mistake!**\n\n**Learn from scotty-37**: Any in-memory cache with TTL **requires** cleanup.","status":"open","priority":2,"issue_type":"task","created_at":"2025-10-27T01:27:25.523931+01:00","updated_at":"2025-10-27T01:29:42.036307+01:00","labels":["caching","oauth","performance","scalability"]}
{"id":"scotty-5","title":"Enhanced monitoring and observability","description":"Add comprehensive monitoring for the unified output system. Basic logging exists but metrics and tracing are incomplete.","design":"Implementation using OpenTelemetry Collector + VictoriaMetrics architecture.\n\nArchitecture:\n- Scotty exports OTLP metrics to OTel Collector (port 4317)\n- OTel Collector routes traces to Jaeger, metrics to VictoriaMetrics\n- Grafana visualizes metrics from VictoriaMetrics\n- Total memory overhead: ~180-250 MB\n\nSee docs/research/otel-metrics-backend-evaluation.md for complete research and rationale.","acceptance_criteria":"- Prometheus metrics exported\n- Grafana dashboard created\n- Tracing spans for log/shell operations\n- Memory usage tracked\n- Error rates monitored","notes":"Chosen solution: OTel Collector + VictoriaMetrics for lightweight, OpenTelemetry-native metrics collection. Integrates with existing Jaeger setup.","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-25T00:58:22.135422+02:00","updated_at":"2025-10-25T01:28:15.468691+02:00","dependencies":[{"issue_id":"scotty-5","depends_on_id":"scotty-1","type":"parent-child","created_at":"2025-10-25T00:58:32.652182+02:00","created_by":"daemon"}]}
{"id":"scotty-6","title":"End-user documentation for unified output system","description":"Write comprehensive end-user documentation for the logs and shell features. Technical PRD exists but user-facing docs are missing.","design":"- User guide for app:logs command with examples\n- User guide for app:shell command (once CLI implemented)\n- Web UI documentation for log viewer\n- Web UI documentation for shell access\n- Security best practices for shell access\n- Troubleshooting guide\n- Add to main documentation site","acceptance_criteria":"- app:logs documented with all options\n- app:shell documented (when available)\n- Screenshots of web UI features\n- Security guidelines clear\n- Common issues documented\n- Published to docs site","status":"open","priority":3,"issue_type":"task","created_at":"2025-10-25T00:58:22.241151+02:00","updated_at":"2025-10-25T00:58:22.241151+02:00","dependencies":[{"issue_id":"scotty-6","depends_on_id":"scotty-1","type":"parent-child","created_at":"2025-10-25T00:58:32.723437+02:00","created_by":"daemon"},{"issue_id":"scotty-6","depends_on_id":"scotty-2","type":"related","created_at":"2025-10-25T00:58:39.470922+02:00","created_by":"daemon"}]}
{"id":"scotty-7","title":"Add OpenTelemetry metrics dependencies to workspace","description":"Add metrics feature to opentelemetry and opentelemetry-otlp crates in workspace Cargo.toml. Enable metrics support in opentelemetry_sdk.","design":"Update workspace Cargo.toml:\n- opentelemetry: Add \"metrics\" feature\n- opentelemetry_sdk: Add \"metrics\" to features array\n- opentelemetry-otlp: Add \"metrics\" feature","acceptance_criteria":"- Cargo.toml updated with metrics features\n- cargo check passes\n- No breaking changes to existing trace functionality","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-25T01:28:15.598548+02:00","updated_at":"2025-10-25T01:32:41.399679+02:00","closed_at":"2025-10-25T01:32:41.399679+02:00","dependencies":[{"issue_id":"scotty-7","depends_on_id":"scotty-5","type":"parent-child","created_at":"2025-10-25T01:28:29.940257+02:00","created_by":"daemon"}]}
{"id":"scotty-8","title":"Create metrics module with ScottyMetrics struct","description":"Create scotty/src/metrics/mod.rs with ScottyMetrics struct containing all metric instruments (counters, gauges, histograms) for unified output system monitoring.","design":"Create metrics module with:\n- ScottyMetrics struct with all instruments\n- init_metrics() function to set up OTLP exporter\n- Metrics for: log streams, shell sessions, WebSocket, tasks, system health\n- Uses opentelemetry::metrics API (Counter, Gauge, Histogram)","acceptance_criteria":"- metrics/mod.rs created and compiles\n- ScottyMetrics struct has all planned metrics\n- init_metrics() successfully initializes MeterProvider\n- Metrics can be recorded without panics","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-25T01:28:15.721881+02:00","updated_at":"2025-10-25T01:50:03.687196+02:00","closed_at":"2025-10-25T01:50:03.687196+02:00","dependencies":[{"issue_id":"scotty-8","depends_on_id":"scotty-5","type":"parent-child","created_at":"2025-10-25T01:28:30.027423+02:00","created_by":"daemon"},{"issue_id":"scotty-8","depends_on_id":"scotty-7","type":"blocks","created_at":"2025-10-25T01:28:42.019675+02:00","created_by":"daemon"}]}
{"id":"scotty-9","title":"Instrument log streaming service with metrics","description":"Add metrics recording to LogStreamingService for stream counts, durations, errors, and bytes transferred.","design":"Instrument LogStreamingService:\n- Increment log_streams_active on stream start\n- Increment log_streams_total counter\n- Record log_stream_duration on completion\n- Track log_lines_received and log_stream_bytes\n- Increment log_stream_errors on failures","acceptance_criteria":"- Metrics recorded at stream start/end\n- Duration measured accurately\n- Error cases increment error counter\n- No performance degradation","status":"closed","priority":1,"issue_type":"task","created_at":"2025-10-25T01:28:15.847014+02:00","updated_at":"2025-10-25T01:57:04.063722+02:00","closed_at":"2025-10-25T01:57:04.063722+02:00","dependencies":[{"issue_id":"scotty-9","depends_on_id":"scotty-5","type":"parent-child","created_at":"2025-10-25T01:28:30.109241+02:00","created_by":"daemon"},{"issue_id":"scotty-9","depends_on_id":"scotty-8","type":"blocks","created_at":"2025-10-25T01:28:42.089191+02:00","created_by":"daemon"}]}
